

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Hopfield Networks &#8212; Computing in Physics (Phy446)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ML/Hopfield';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Restricted Boltzmann Machines" href="RBM.html" />
    <link rel="prev" title="Overview" href="Overview.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../Overview.html">
  
  
  
  
  
  
    <p class="title logo__title">Computing in Physics (Phy446)</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Setting up</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted.html">Getting Setup</a></li>






</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cellular Automata</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../CellularAutomata/CellularAutomata.html">Cellular Automata</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CellularAutomata/Sand.html">Sand</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CellularAutomata/OtherAutomata.html">Other Interesting Automata</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Quantum Computing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../QC/Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../QC/0-DiracNotation.html">Dirac Notation</a></li>

<li class="toctree-l1 has-children"><a class="reference internal" href="../QC/1a-QuantumComputingSimulator.html">QC Simulators</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../QC/SimulatorS.html">Simulator S</a></li>
<li class="toctree-l2"><a class="reference internal" href="../QC/1b-QuantumComputingSimulator.html">Simulator M-(abcd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../QC/Measuring.html">Measuring and Input</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../QC/NonAtomicGates.html">Non-atomic gates</a></li>

<li class="toctree-l1"><a class="reference internal" href="../QC/PhaseEstimation.html">Phase estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../QC/QFT.html">Quantum Fourier Transform</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../QC/Shor-Overview.html">Shor’s Algorithm</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../QC/Shor-Classical.html">Shor’s Algorithm (classically)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../QC/Shor-Quantum.html">Quantum Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../QC/ModularMultiplication.html">Modular Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../QC/Shor-QuantumCircuit.html">Shor’s Algorithm</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Quantum Computing (extensions)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../QC/Gates.html">Gates</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../QC/ClassicalGates.html">Classical Gates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../QC/ControlledGates.html">Controlled Gates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../QC/Universal.html">Gates for any Unitary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../QC/BQPinPSPACE.html">BQP in PSPACE</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Ising Model</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Ising/Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ising/IsingModel.html">Simulating an Ising Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ising/Measure.html">Measuring the Ising Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Ising/RG.html">The Renormalization Group</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Ising/SimulatedAnnealing.html">Extra Credit: Simulated Annealing</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Ising/ProteinFolding.html">Protein Folding</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Overview.html">Overview</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Hopfield Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="RBM.html">Restricted Boltzmann Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="Diffusion.html">Generative Diffusion Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topological Insulators</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../TI/Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../TI/Lattice.html">Lattices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../TI/TightBinding.html">Tight Binding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../TI/ChernInsulators.html">Topological Insulators</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Hopfield Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hopfield-networks-as-an-ising-model">Hopfield networks as an Ising Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-hopfield-network">Building a Hopfield Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seeing-that-the-energy-goes-down-during-a-run">Seeing that the “energy” goes down during a run</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-some-inputs">Training some inputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-many-memories-can-you-remember">How many memories can you remember?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#looking-at-the-energy-landscape">Looking at the energy landscape</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-efficient-training-images">More efficient training (images)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="hopfield-networks">
<h1>Hopfield Networks<a class="headerlink" href="#hopfield-networks" title="Permalink to this heading">#</a></h1>
<p>The first aspect of the brain that we will try to understand is memory. How does memory work? Roughly speaking, neurons somehow store sensational signals entering the brain as memories, then “remember” those memories by retrieving them when similar signals appear in the future.</p>
<p>The Hopfield network, invented by the physicist John Hopfield, is a model of how neurons store and process memories. We will learn how to implement this model, teach it to memorize images, and recover images from its memory.</p>
<p>The Hopfield network is a particular artificial neural network. In general, a neural network is a graph with neurons at the vertices of the graph and edges connecting the neurons to one another. The edges of this graph can be directed or undirected. The Hopfield network in particular has undirected edges. The Hopfield network has a set of variables that define the model. Each neuron <span class="math notranslate nohighlight">\(i\)</span> has a <em>state</em> <span class="math notranslate nohighlight">\(s_i\)</span> that represents the level of activation of the neuron. Additionally, each neuron has a bias (or threshold) <span class="math notranslate nohighlight">\(b_i\)</span> to activate and each edge has a weight <span class="math notranslate nohighlight">\(W_{ij}\)</span> associated with it that indicates how likely neurons <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are to activate together.</p>
<p>The Hopfield network is a dynamical model. Its dynamics is governed by an update rule. At step <span class="math notranslate nohighlight">\(n\)</span> in the dynamics, we choose a random neuron <span class="math notranslate nohighlight">\(i\)</span> and update it state. The state of the <span class="math notranslate nohighlight">\(i\)</span>-th neuron in the network has <span class="math notranslate nohighlight">\(s^n_i\)</span> is either -1 or 1. To decide on the new state at step <span class="math notranslate nohighlight">\(n+1\)</span>, we:</p>
<ul class="simple">
<li><p>ignore the neuron’s current state,</p></li>
<li><p>compute the total “weight” coming into it, <span class="math notranslate nohighlight">\(T_i \equiv \sum_{j\neq i}  s^n_j W_{ji}\)</span>,</p></li>
<li><p>if <span class="math notranslate nohighlight">\(T_i &gt; b_i\)</span> then the new state is <span class="math notranslate nohighlight">\(s^{n+1}_i = 1\)</span>  else <span class="math notranslate nohighlight">\(s^{n+1}_i = -1\)</span></p></li>
</ul>
<p>Here is a sketch of a four neuron Hopfield network in a particular state. Included is a description of how to update the circled neuron’s state according to the above update rule.</p>
<p><img alt="" src="../_images/diagram.jpg" /></p>
<hr class="docutils" />
<section id="hopfield-networks-as-an-ising-model">
<h2>Hopfield networks as an Ising Model<a class="headerlink" href="#hopfield-networks-as-an-ising-model" title="Permalink to this heading">#</a></h2>
<p>The Hopfield network is really just an Ising model at zero temperature with a site-dependent magnetic field and long-ranged interactions. The analogous concepts are:</p>
<p>Hopfield Network <span class="math notranslate nohighlight">\(\longleftrightarrow\)</span> Ising Model</p>
<ul class="simple">
<li><p>Neuron <span class="math notranslate nohighlight">\(\longleftrightarrow\)</span> Ising Spin</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(+1\)</span> = Spin Up</p></li>
<li><p><span class="math notranslate nohighlight">\(-1\)</span> = Spin Down</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(W_{ij}\)</span> <span class="math notranslate nohighlight">\(\longleftrightarrow\)</span> <span class="math notranslate nohighlight">\(J_{ij}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_i\)</span> <span class="math notranslate nohighlight">\(\longleftrightarrow\)</span> <span class="math notranslate nohighlight">\(h_i\)</span></p></li>
</ul>
<p>The energy of the corresponding Ising model is</p>
<div class="math notranslate nohighlight">
\[
E = -\frac{1}{2} \sum_{ij} W_{ij} s_i s_j + \sum_i b_i s_i.
\]</div>
<p>Recall how we did Metropolis Markov chain Monte Carlo updates in the Ising model unit. We chose a spin at random and flipped it with a probability <span class="math notranslate nohighlight">\(\min(1,e^{-\Delta E/k_BT})\)</span> that depended on the energy of the old and new spin configurations. When the temperature is zero (<span class="math notranslate nohighlight">\(T=0\)</span>), we only accept spin flips that decreased the energy. This <span class="math notranslate nohighlight">\(T=0\)</span> Monte Carlo spin flip update is equivalent to the update rule of the Hopfield network.</p>
<p>There is a really nice implication of this analogy. First, notice that each update either decreases the energy or leaves it unchanged. This means that after enough updates you will eventually reach a configuration which is a <em>local</em> minimum in energy. At such a minimum, there are no further network updates to make; each attempted update step returns the same configuration. Consequently, for each input to a Hopfield network, there is a particular output to which the Hopfield network converges.</p>
<p>This property of Hopfield networks is reminiscint of memory. For different stimuli, I may remember different things. Our hope is that:</p>
<ul class="simple">
<li><p>those “memories” are close to the original stimulus</p></li>
<li><p>we can find a way to remember particular memories.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="building-a-hopfield-network">
<h2>Building a Hopfield Network<a class="headerlink" href="#building-a-hopfield-network" title="Permalink to this heading">#</a></h2>
<p>The first step of this assignment is to write code to build a Hopfield Network. My recommendation is to take an object-oriented approach, making a Hopfield network class that keeps the states <span class="math notranslate nohighlight">\(s_i\)</span> and biases <span class="math notranslate nohighlight">\(b_i\)</span> of the <span class="math notranslate nohighlight">\(n\)</span> neurons in two vectors and the weights <span class="math notranslate nohighlight">\(W_{ij}\)</span> in an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix. You will need a function that updates the state (chooses a random neuron and updates it according to the above update rule). You will also need a procedure to determine if the network has converged. One way to go about this is to loop through all the neurons and check if any of them want to change (but don’t change them!). A function that computes the energy, as well as functions that set and return the state, are useful as well.</p>
<p>To set up the bias, choose a random number between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> for each <span class="math notranslate nohighlight">\(b_i\)</span>.</p>
<p>To set up the weights, choose a random number between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> for each <span class="math notranslate nohighlight">\(W_{ij}\)</span>. Remember that the hopfield network is symmetric so make sure that <span class="math notranslate nohighlight">\(W_{ij}=W_{ji}\)</span> <!-- and make sure that $W_{ii}=0$.--></p>
<p>Once you have a class for the Hopfield network, set random weights and biases, and a random state and update it until it converges. At the moment, because we haven’t learned how to train the network, it’s not clear whether or not the results are sensible.</p>
</section>
<hr class="docutils" />
<section id="seeing-that-the-energy-goes-down-during-a-run">
<h2>Seeing that the “energy” goes down during a run<a class="headerlink" href="#seeing-that-the-energy-goes-down-during-a-run" title="Permalink to this heading">#</a></h2>
<p>As the next step, we’d like to explicitly verify the statement that the energy never increases during a Hopfield network’s dynamics. Run your network through the update rule for a series of random initial states. Measure the energy of the initial states and their subsequent updates. Produce a graph, with lines for each run, plotting the energy versus update step. Show that the plots are monotonically non-increasing. This demonstrates that Hopfield networks converge to some “memory” for different initial conditions.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Produce a graph with 10 lines on it showing that the energy starting from 10 different intial conditions decays until convergence. It should look something like:</p>
<p><img alt="energy2.jpg" src="../_images/energy2.jpg" /></p>
<p>*Hint: * This was produced with 100 neurons. I ran 10 different initial conditions and ran each one until they converged. I printed out the energy every 100 update steps (i.e., equal to the number of neurons).</p>
</div>
</section>
<hr class="docutils" />
<section id="training-some-inputs">
<h2>Training some inputs<a class="headerlink" href="#training-some-inputs" title="Permalink to this heading">#</a></h2>
<p>Now that you have a Hopfield network running, the next step is to learn how to train it to memorize some data. Hopfield networks can remember arbitrary bit-strings. In our case, these bit-strings will represent binary images, where <span class="math notranslate nohighlight">\(\pm 1\)</span> represent different color pixels in the image.</p>
<p>To memorize a set of <span class="math notranslate nohighlight">\(m\)</span> memories, we need to set our weights to</p>
<div class="math notranslate nohighlight">
\[
W = \frac{1}{m} \sum_{k=1}^m v^{(k)} (v^{(k)})^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\(v^{(k)}\)</span> is an <span class="math notranslate nohighlight">\(n \times 1\)</span> column vector representing the <span class="math notranslate nohighlight">\(k\)</span>-th memory.</p>
<p>Let’s have it learn some images (we will start with some small images and then work up to doing bigger images). Take the images generated in <a class="reference download internal" download="" href="../_downloads/b39008f4ba14faefed13c585b3f5eeaa/makeImage.py"><span class="xref download myst">makeImage.py</span></a>. These are just binary numbers that can be converted into the state of the neurons.</p>
<p>Some example images to remember:
<img alt="Face" src="../_images/face2.jpg" />
<strong>Binary Number:</strong> 0000000000000100010000000000000000000000000010000000000000000001110000001000100001000001101000000001</p>
<p><img alt="Tree" src="../_images/tree2.jpg" />
<strong>Binary Number:</strong> 0001111000000111100000001100000000110000001111111000001100100000110000000011000000001100000000110000</p>
<p>Here I am using “0” and “1” which gets mapped to “-1” and “1”.</p>
<p>Write code that reads in these images and sets the weights of a Hopfield network to remember them. We want to see if the network can restore these images. To do this, we will feed it a “nearby” image and see how well it recovers the remembered image. Use these two approaches to give it a nearby image:</p>
<ul class="simple">
<li><p>Take the face (or the tree) image and remove (or corrupt somehow) the left 1/4 of it. Save the resulting binary string and run a Hopfield network — trained on the uncorrupted image — with this as the input state. Take the binary string produced at the end of the simulation and see what the result is (by comparing both the output binary number and the picture it represents). You might want to print occassional snapshots of the picture as the network updates.</p></li>
<li><p>Write code that takes a string and perturbs it in <span class="math notranslate nohighlight">\(k\)</span> spots. Then feed it to your network and let it evolve and see if you can get it back.</p></li>
</ul>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Show that you can memorize these images and restore them after you’ve perturbed them.</p>
</div>
</section>
<hr class="docutils" />
<section id="how-many-memories-can-you-remember">
<h2>How many memories can you remember?<a class="headerlink" href="#how-many-memories-can-you-remember" title="Permalink to this heading">#</a></h2>
<p>In this section, we will determine how many memories a Hopfield network can remember. To do this, write code that generates <span class="math notranslate nohighlight">\(p\)</span> memories and encodes their weights into a network. Then pick one of those target memories and corrupt it with <span class="math notranslate nohighlight">\(k\)</span> random bit flips. (When corrupting the bits of this memory, pick <span class="math notranslate nohighlight">\(k\)</span> distinct random neurons and flip their states. You can do this with <code class="docutils literal notranslate"><span class="pre">numpy.random.shuffle()</span></code>. Do not try to randomly pick neurons one at a time since you might “corrupt” a neuron multiple times this way.) Do this enough times for each <span class="math notranslate nohighlight">\((p,k)\)</span> to compute a reasonable average and produce a matrix of the <a class="reference external" href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a> between the target and the converged result. This will be a <span class="math notranslate nohighlight">\(p \times k\)</span> matrix which can be displayed in python using <code class="docutils literal notranslate"><span class="pre">pylab.matshow</span></code>.</p>
<p>Use 100 neurons. You probably want to use the version of your code which stores weights as a matrix.</p>
<p>You should be able to cleanly see how many memories the network can remember and how robust it is at remembering a memory given a partial version of that memory.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Produce a graph that shows how many memories you can remember.
<img alt="hammingDistance" src="../_images/hammingDistance.jpg" /></p>
<p>*Hint: *To produce this graph I used 100 neurons.  For each point on this graph (memories x corrupted bits) I stored 5 sets of memories and attempted to recover a random (perturbed) memory from that set 20 times.  I then computed the average Hamming distances between the memory I got and the memory I was trying to recover over these 100 attempts.</p>
</div>
</section>
<hr class="docutils" />
<section id="looking-at-the-energy-landscape">
<h2>Looking at the energy landscape<a class="headerlink" href="#looking-at-the-energy-landscape" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>To accomplish this part of the assignment you need to download <a class="reference external" href="https://www.graphviz.org/">Graphviz</a> or use the online version of graphviz (which is probably easier).</p>
</div></blockquote>
<p>In this step we will to look at the energy landscape of the Hopfield network more explicitly. A Hopfield network with <span class="math notranslate nohighlight">\(n\)</span> neurons has <span class="math notranslate nohighlight">\(2^n\)</span> possible {on,off} combinations of neuron states. Each of these combinations can flow down in energy to (at most) <span class="math notranslate nohighlight">\(n\)</span> different nodes after attempting to update one of the neurons. This generates a graph.</p>
<p>Give your hopfield network 7 neurons and 2 memories.</p>
<p>Graphviz eats a description of a graph and generates a picture of it.
The description looks like</p>
<p>Generate this graph, where a directed arrow corresponds to one configuration pointing to another configuration. For your first attempt, just color the target memories.</p>
<p>To compute a picture from this do</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dot</span> <span class="n">myFile</span><span class="o">.</span><span class="n">digraph</span>  <span class="o">-</span><span class="n">Tsvg</span> <span class="o">-</span><span class="n">o</span> <span class="n">myFile</span><span class="o">.</span><span class="n">svg</span>
</pre></div>
</div>
<p>You can then load a svg file in your browser.</p>
<p>Another way to understand the energy landscape is to use the energies. In python you can do something like</p>
<div class="highlight-{.python notranslate"><div class="highlight"><pre><span></span>a=numpy.loadtxt(&quot;energies&quot;)
ax = pylab.axes()
ax.plot(a[:,0],a[:,1],&#39;o&#39;)
for i in range(0,len(a[:,0])):
    myDict[int(a[i,0])]=a[i,1]

b=numpy.loadtxt(&quot;connections&quot;)
for i in range(0,len(b[:,0])):
    before=int(b[i,0])
    after=int(b[i,1])
    pylab.plot([before,after],[myDict[before],myDict[after]],&#39;--&#39;)
</pre></div>
</div>
<p><em>Hint:</em> To do this, I used 6 neurons.  This means <span class="math notranslate nohighlight">\(2^6\)</span> states. For each state I looped over the 6 neurons and produced all of the states that I might get from doing an update of that neuron (to do this I needed to write a <code class="docutils literal notranslate"><span class="pre">Update(int</span> <span class="pre">neuron)</span></code> function that eats a neuron to try to update. Then I printed out the two integers that represent the binary numbers before and after the update.</p>
<p>If you’re using C++, here’s one way of mapping binary number strings to integers.</p>
<div class="highlight-{.numberLines notranslate"><div class="highlight"><pre><span></span> int theSize=6;
  string targetString=&quot;&quot;;
  string oneString=&quot;&quot;;
  for (int i=0;i&lt;theSize;i++){
    targetString=targetString+&quot;0&quot;;
    oneString=oneString+&quot;1&quot;;
  }

  map&lt;string,int&gt; myStrings;
  myStrings.insert(make_pair(targetString,0));
  int count=0;
  while (targetString!=oneString){
    int digit=0;
    while (targetString[digit]==&#39;1&#39;)
      digit++;
    if (digit!=theSize){
      targetString[digit]=&#39;1&#39;;
      for (int i=0;i&lt;digit;i++){
        targetString[i]=&#39;0&#39;;
      }
    }
    count++;
    myStrings.insert(make_pair(targetString,count));
  }
</pre></div>
</div>
<p>Then to print out the strings do something like</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">myStrings</span><span class="p">[</span><span class="n">string1</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot; -&gt; &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">myStrings</span><span class="p">[</span><span class="n">string2</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot;;&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>My energy landscape pictures look a bit like this:
<img alt="" src="../_images/energy.svg" /></p>
<p><img alt="" src="../_images/energyLandscape.jpg" /></p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Show the energy landscape.</p>
</div>
</section>
<hr class="docutils" />
<section id="more-efficient-training-images">
<h2>More efficient training (images)<a class="headerlink" href="#more-efficient-training-images" title="Permalink to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Extra Credit</p>
<p><strong>This section is extra credit (5 points)!</strong></p>
<p>Our images are a bit pixelated.  It would be nice to put more sophisticated images into memory, like a 64x64 gray-scale thumbnail.  Unfortunately such a thumbnail has 64x64x8 bits. Can the Hopfield network remember an image of that size?</p>
<p>Understanding the computational efficiency of algorithms is one of the running themes of this class. The Hopfield network memory algorithm  stores a weight matrix which constitutes the majority of the algorithm’s space complexity, scaling as <span class="math notranslate nohighlight">\(O(n^2)\)</span> (<span class="math notranslate nohighlight">\(2^{30}\)</span> numbers for the 64x64 greyscale image). Concerning time complexity, the algorithm’s efficiency is <span class="math notranslate nohighlight">\(O(n)\)</span> (<em>you should understand why this is!</em>). Storage is the algorithm’s primary limitation.</p>
<p>Given how we are training, let’s figure out if there’s a more efficient way to use space. Notice how updating a neuron uses only <span class="math notranslate nohighlight">\(n\)</span> values of <span class="math notranslate nohighlight">\(W_{ij}\)</span>. Suppose you have a set of <span class="math notranslate nohighlight">\(p\)</span> memories to store, represented by the vectors <span class="math notranslate nohighlight">\(\{ v^{(k)} \}\)</span>, and want to find a particular weight <span class="math notranslate nohighlight">\(W_{ij}\)</span>. Figure out how to reduce the space complexity to <span class="math notranslate nohighlight">\(O(pn)\)</span> in exchange for an increase in time complexity to <span class="math notranslate nohighlight">\(O(pn)\)</span>. <strong>Copy your code before implementing these changes because the old code (with weights as a matrix) will still be useful</strong>.</p>
<p><strong>Snapshots of pictures during the neural net (extra credit)</strong></p>
<p>Now you should be able to take thumbnails and remember them, modify them somehow and then recall them.
To help you, we have some python code <a class="reference download internal" download="" href="../_downloads/977d62fd7729e36da01ffcaf0635ea75/img2.py"><span class="xref download myst">here</span></a> which</p>
<ul class="simple">
<li><p>takes an image and generates its binary number representation.</p></li>
<li><p>takes a binary number and generate its image representation.</p></li>
</ul>
<p>Go ahead and learn some images (more then one). Run your Hopfield network with a corrupted version of one of these images as input and take some snapshots as your network updates. Make sure to take a snapshot of final state it converges to.</p>
<div class="caution admonition">
<p class="admonition-title">Grading</p>
<p>Measure a few (say 2) thumbnail images. Then input one of the partially corrupted thumbnail images and show that if you run your hopfield network, it recovers the uncorrupted image.  This uses <span class="math notranslate nohighlight">\(n=64 \times 64 \times 8\)</span> neurons. Perturb by <span class="math notranslate nohighlight">\(64 \times 8 \times 20\)</span> states. Output a snapshot every <span class="math notranslate nohighlight">\(1000\)</span> updates. This takes a long time to run.</p>
</div>
</div>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ML"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Overview.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Overview</p>
      </div>
    </a>
    <a class="right-next"
       href="RBM.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Restricted Boltzmann Machines</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hopfield-networks-as-an-ising-model">Hopfield networks as an Ising Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-hopfield-network">Building a Hopfield Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seeing-that-the-energy-goes-down-during-a-run">Seeing that the “energy” goes down during a run</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-some-inputs">Training some inputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-many-memories-can-you-remember">How many memories can you remember?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#looking-at-the-energy-landscape">Looking at the energy landscape</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-efficient-training-images">More efficient training (images)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Bryan Clark
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>